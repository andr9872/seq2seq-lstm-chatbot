{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0+cu102\n"
     ]
    }
   ],
   "source": [
    "# Check current torch version\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.43.0)\n"
     ]
    }
   ],
   "source": [
    "# Intall tqdm for progress bars\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Build your Vocabulary & create the Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import torch\n",
    "from nltk.corpus import brown\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Loading provided embedding\n",
    "w2v = gensim.models.Word2Vec.load('brown.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Due to ongoing problems with torchtext, the SQuAD2.0 dataset was obtained directly from\n",
    "# https://rajpurkar.github.io/SQuAD-explorer/\n",
    "\n",
    "# Load SQuAD2.0 record from root directory\n",
    "with open('train-v2.0.json', 'r') as f:\n",
    "    squad_train = pd.read_json(f)\n",
    "    \n",
    "with open('dev-v2.0.json', 'r') as f:\n",
    "    squad_dev = pd.read_json(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Questions and Answers\n",
    "def extract_qa(data):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for topic in data['data']:\n",
    "        for paragraph in topic['paragraphs']:\n",
    "            for qas in paragraph['qas']:\n",
    "                questions.append(qas['question'])\n",
    "                if qas['answers']:\n",
    "                    answers.append(qas['answers'][0]['text'])\n",
    "                else: \n",
    "                    answers.append('') # No-answer\n",
    "    return questions, answers\n",
    "\n",
    "train_questions, train_answers = extract_qa(squad_train)\n",
    "dev_questions, dev_answers = extract_qa(squad_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When did Beyonce start becoming popular?\n",
      "in the late 1990s\n",
      "In what country is Normandy located?\n",
      "France\n"
     ]
    }
   ],
   "source": [
    "# Check sample questions and answers\n",
    "print(train_questions[0])\n",
    "print(train_answers[0])\n",
    "print(dev_questions[0])\n",
    "print(dev_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing questions: 100%|██████████| 130319/130319 [00:27<00:00, 4799.08it/s]\n",
      "Tokenizing answers: 100%|██████████| 130319/130319 [00:13<00:00, 9356.59it/s] \n",
      "Tokenizing questions: 100%|██████████| 11873/11873 [00:02<00:00, 4722.63it/s]\n",
      "Tokenizing answers: 100%|██████████| 11873/11873 [00:01<00:00, 11663.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize using NLTK\n",
    "def tokenize_data(questions, answers):\n",
    "    questions_tokens = []\n",
    "    answers_tokens = []\n",
    "    \n",
    "    for question in tqdm(questions, desc=\"Tokenizing questions\"):\n",
    "        questions_tokens.append(word_tokenize(question))\n",
    "    \n",
    "    for answer in tqdm(answers, desc=\"Tokenizing answers\"):\n",
    "        answers_tokens.append(word_tokenize(answer))\n",
    "        \n",
    "    return questions_tokens, answers_tokens\n",
    "\n",
    "train_questions_tokens, train_answers_tokens = tokenize_data(train_questions, train_answers)\n",
    "dev_questions_tokens, dev_answers_tokens = tokenize_data(dev_questions, dev_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Vocabulary\n",
    "all_tokens = [token for sublist in train_questions_tokens+train_answers_tokens for token in sublist]\n",
    "vocab = set(all_tokens)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73078"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Vocabulary: 100%|██████████| 260638/260638 [00:00<00:00, 418390.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create Word-Index Mapping\n",
    "word2index = {}\n",
    "index2word = {}\n",
    "\n",
    "special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "\n",
    "for token in special_tokens:\n",
    "    word2index[token] = len(word2index)\n",
    "    index2word[len(index2word)] = token\n",
    "\n",
    "for tokens in tqdm(train_questions_tokens + train_answers_tokens, desc=\"Building Vocabulary\"):\n",
    "    for word in tokens:\n",
    "        if word not in word2index:\n",
    "            word2index[word] = len(word2index)\n",
    "            index2word[len(index2word)] = word\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73082"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Embeddings\n",
    "embedding_dim = w2v.vector_size\n",
    "embedding_matrix = np.random.uniform(-1, 1, (len(word2index), embedding_dim))\n",
    "\n",
    "for word, i in word2index.items():\n",
    "    try:\n",
    "        if word in special_tokens:\n",
    "            continue\n",
    "        embedding_vector = w2v.wv[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        # If word not in pretrained embeddings (alternatively: pass)\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN or infinities\n",
    "if np.isnan(embedding_matrix).any():\n",
    "    print(\"NaN found in embedding matrix\")\n",
    "if np.isinf(embedding_matrix).any():\n",
    "    print(\"Infinity found in embedding matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, input_step, hidden, cell):\n",
    "        embedded = self.embedding(input_step).unsqueeze(0)  # Now it's [1, batch_size, embedding_dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        output = self.out(output.squeeze(0))\n",
    "        return output, (hidden, cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Combine them into a Seq2Seq Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, source, target):\n",
    "        # Placeholder for decoder outputs\n",
    "        outputs = torch.zeros(target.size(0), target.size(1), self.decoder.out.out_features).to(target.device)\n",
    "        \n",
    "        # Pass input through the encoder\n",
    "        hidden, cell = self.encoder(source)\n",
    "        \n",
    "        # First input to the decoder is the <sos> token\n",
    "        input = target[:, 0]\n",
    "        \n",
    "        for t in range(1, target.size(1)):\n",
    "            # Note: decoder should accept input with shape [batch_size, 1] \n",
    "            # but seems your decoder is set up to take [batch_size], \n",
    "            # hence not using unsqueeze\n",
    "            output, (hidden, cell) = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t] = output\n",
    "            # No teacher forcing: use model's prediction as next input\n",
    "            input = output.argmax(1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "hidden_dim = 128\n",
    "encoder = Encoder(vocab_size, embedding_dim, hidden_dim)\n",
    "decoder = Decoder(vocab_size, embedding_dim, hidden_dim)\n",
    "seq2seq = Seq2Seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Train & evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Tokens to Indices\n",
    "def tokens_to_indices(tokens_list, word2index):\n",
    "    return [[word2index[token] for token in tokens] for tokens in tokens_list]\n",
    "\n",
    "train_questions_indices = tokens_to_indices(train_questions_tokens, word2index)\n",
    "train_answers_indices = tokens_to_indices(train_answers_tokens, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to available device, set optimizer with scheduler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "seq2seq = seq2seq.to(device)\n",
    "optimizer = optim.Adam(seq2seq.parameters(), lr=0.001) # Lowered to avoid exploding values\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)  # Decrease the learning rate by a factor of 0.1 every 5 epochs\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2index['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1000/1000 [08:46<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1000/1000 [08:49<00:00,  1.89it/s]\n",
      "Epoch 3/3:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1000/1000 [08:52<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Loop (For simplicity Batch size of 1)\n",
    "epochs = 3  # Due to ongoing issues with the Udacity GPU workspace and low speeds, \n",
    "# unfortunately more epochs could not be tried out\n",
    "max_len = 100  # To adjust the average sequence length\n",
    "limit_data = 1000  # To train with only a portion of the data when resources are limited\n",
    "# the max would be: len(train_questions_indices)\n",
    "\n",
    "# Set model to train state\n",
    "seq2seq.train()  \n",
    "\n",
    "# Initialize a variable to keep track of the best loss so far\n",
    "best_loss = float('inf')\n",
    "model_save_path = 'best_seq2seq_model.pt'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i in tqdm(range(min(len(train_questions_indices), limit_data)), desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        question = train_questions_indices[i]\n",
    "        answer = train_answers_indices[i]\n",
    "        # Note: for debugging purposes a few print statements were added, \n",
    "        # which are now commented out again\n",
    "        # print(\"Question (indices):\", question)\n",
    "        # print(\"Answer (indices):\", answer)\n",
    "        # print(\"Length of question before slicing:\", len(question))\n",
    "        # print(\"Length of answer before slicing:\", len(answer))\n",
    "        \n",
    "        # Pad or truncate to max_len\n",
    "        question = question[:max_len] + [word2index['<pad>']] * (max_len - len(question))\n",
    "        answer = answer[:max_len] + [word2index['<pad>']] * (max_len - len(answer))\n",
    "        \n",
    "        # Convert list of indices to tensors\n",
    "        question_tensor = torch.LongTensor(question).unsqueeze(0).to(device)\n",
    "        answer_tensor = torch.LongTensor(answer).unsqueeze(0).to(device)\n",
    "        # print(\"Before reshaping:\")\n",
    "        # print(\"Answer tensor shape:\", answer_tensor.shape)\n",
    "        # print(\"Answer tensor:\", answer_tensor)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = seq2seq(question_tensor, answer_tensor)\n",
    "        #print(\"Outputs shape:\", outputs.shape)\n",
    "        #print(\"Outputs:\", outputs)\n",
    "        outputs = outputs[:, 1:].reshape(-1, outputs.shape[2])\n",
    "        answer_tensor = answer_tensor[:, 1:].reshape(-1)\n",
    "        \n",
    "        #print(\"After reshaping:\")\n",
    "        #print(\"Outputs shape:\", outputs.shape)\n",
    "        #print(\"Answer tensor shape:\", answer_tensor.shape)\n",
    "        \n",
    "        loss = criterion(outputs, answer_tensor)\n",
    "        \n",
    "        '''\n",
    "        for name, param in seq2seq.named_parameters():\n",
    "            if torch.isnan(param).any():\n",
    "                print(f\"NaN found in {name} before backward\")\n",
    "            if torch.isinf(param).any():\n",
    "                print(f\"Infinity found in {name} before backward\")\n",
    "        '''\n",
    "        loss.backward()\n",
    "        '''\n",
    "        for name, param in seq2seq.named_parameters():\n",
    "            if torch.isnan(param.grad).any():\n",
    "                print(f\"NaN found in gradient of {name} after backward\")\n",
    "            if torch.isinf(param.grad).any():\n",
    "                print(f\"Infinity found in gradient of {name} after backward\")\n",
    "        '''\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(seq2seq.parameters(), max_norm=1)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        # print(loss.item())\n",
    "        # print(epoch_loss)\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate average epoch loss\n",
    "    avg_epoch_loss = epoch_loss/min(len(train_questions_indices), limit_data)\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {avg_epoch_loss}\")\n",
    "\n",
    "    # Check if the current model is better than previous best before saving\n",
    "    if epoch == 0:\n",
    "        best_loss = avg_epoch_loss\n",
    "        torch.save(seq2seq.state_dict(), model_save_path)\n",
    "    else:\n",
    "        if avg_epoch_loss < best_loss:\n",
    "            best_loss = avg_epoch_loss\n",
    "            torch.save(seq2seq.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 0 status: No NaN or Inf\n",
      "Answer 0 status: No NaN or Inf\n",
      "Question 1 status: No NaN or Inf\n",
      "Answer 1 status: No NaN or Inf\n",
      "Question 2 status: No NaN or Inf\n",
      "Answer 2 status: No NaN or Inf\n",
      "Question 3 status: No NaN or Inf\n",
      "Answer 3 status: No NaN or Inf\n",
      "Question 4 status: No NaN or Inf\n",
      "Answer 4 status: No NaN or Inf\n",
      "Question 5 status: No NaN or Inf\n",
      "Answer 5 status: No NaN or Inf\n",
      "Question 6 status: No NaN or Inf\n",
      "Answer 6 status: No NaN or Inf\n",
      "Question 7 status: No NaN or Inf\n",
      "Answer 7 status: No NaN or Inf\n",
      "Question 8 status: No NaN or Inf\n",
      "Answer 8 status: No NaN or Inf\n",
      "Question 9 status: No NaN or Inf\n",
      "Answer 9 status: No NaN or Inf\n"
     ]
    }
   ],
   "source": [
    "# Some additional code to check for potential error sources despite limited epoch number\n",
    "def check_for_nan_or_inf(data):\n",
    "    if torch.any(torch.isnan(data)):\n",
    "        return \"Contains NaN\"\n",
    "    if torch.any(torch.isinf(data)):\n",
    "        return \"Contains Inf\"\n",
    "    return \"No NaN or Inf\"\n",
    "\n",
    "# Just test on a small subset to check for potential issues\n",
    "for i in range(10):  \n",
    "    question = train_questions_indices[i]\n",
    "    answer = train_answers_indices[i]\n",
    "    question = question[:max_len] + [word2index['<pad>']] * (max_len - len(question))\n",
    "    answer = answer[:max_len] + [word2index['<pad>']] * (max_len - len(answer))\n",
    "    question_tensor = torch.LongTensor(question).unsqueeze(0).to(device)\n",
    "    answer_tensor = torch.LongTensor(answer).unsqueeze(0).to(device)\n",
    "    \n",
    "    print(f\"Question {i} status: {check_for_nan_or_inf(question_tensor)}\")\n",
    "    print(f\"Answer {i} status: {check_for_nan_or_inf(answer_tensor)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output status: No NaN or Inf\n",
      "Loss status: Contains NaN\n",
      "Gradient of encoder.embedding.weight status: No NaN or Inf\n",
      "Gradient of encoder.lstm.weight_ih_l0 status: No NaN or Inf\n",
      "Gradient of encoder.lstm.weight_hh_l0 status: No NaN or Inf\n",
      "Gradient of encoder.lstm.bias_ih_l0 status: No NaN or Inf\n",
      "Gradient of encoder.lstm.bias_hh_l0 status: No NaN or Inf\n",
      "Gradient of decoder.embedding.weight status: No NaN or Inf\n",
      "Gradient of decoder.lstm.weight_ih_l0 status: No NaN or Inf\n",
      "Gradient of decoder.lstm.weight_hh_l0 status: No NaN or Inf\n",
      "Gradient of decoder.lstm.bias_ih_l0 status: No NaN or Inf\n",
      "Gradient of decoder.lstm.bias_hh_l0 status: No NaN or Inf\n",
      "Gradient of decoder.out.weight status: No NaN or Inf\n",
      "Gradient of decoder.out.bias status: No NaN or Inf\n"
     ]
    }
   ],
   "source": [
    "seq2seq.train()\n",
    "\n",
    "# Use only one data point for simplicity\n",
    "question = train_questions_indices[0]\n",
    "answer = train_answers_indices[0]\n",
    "question = question[:max_len] + [word2index['<pad>']] * (max_len - len(question))\n",
    "answer = answer[:max_len] + [word2index['<pad>']] * (max_len - len(answer))\n",
    "question_tensor = torch.LongTensor(question).unsqueeze(0).to(device)\n",
    "answer_tensor = torch.LongTensor(answer).unsqueeze(0).to(device)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "outputs = seq2seq(question_tensor, answer_tensor)\n",
    "print(f\"Model output status: {check_for_nan_or_inf(outputs)}\")\n",
    "\n",
    "outputs = outputs[1:].reshape(-1, len(word2index))\n",
    "answer_tensor = answer_tensor[1:].reshape(-1)\n",
    "\n",
    "loss = criterion(outputs, answer_tensor)\n",
    "print(f\"Loss status: {check_for_nan_or_inf(loss)}\")\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "for name, param in seq2seq.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Gradient of {name} status: {check_for_nan_or_inf(param.grad)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Interact with the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(73082, 100)\n",
       "    (lstm): LSTM(100, 128, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(73082, 100)\n",
       "    (lstm): LSTM(100, 128, batch_first=True)\n",
       "    (out): Linear(in_features=128, out_features=73082, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best model\n",
    "seq2seq.load_state_dict(torch.load('best_seq2seq_model.pt'))\n",
    "seq2seq.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Function to Generate Answers\n",
    "def generate_answer(model, question):\n",
    "    # Convert question to tensor\n",
    "    indexed = [word2index.get(word, word2index['<unk>']) for word in question.split()]\n",
    "    question_tensor = torch.LongTensor(indexed).unsqueeze(0).to(device)\n",
    "\n",
    "    # Use the model to get the answer\n",
    "    with torch.no_grad():\n",
    "        outputs = model(question_tensor, torch.zeros_like(question_tensor))\n",
    "    \n",
    "    # Convert tensor outputs to words\n",
    "    answer_indices = outputs.argmax(dim=2).squeeze().cpu().numpy()\n",
    "    answer = ' '.join([index2word[idx] for idx in answer_indices])\n",
    "\n",
    "    # Remove the first occurrence of '<pad>' if present as quick fix\n",
    "    answer = answer.replace('<pad> ', '', 1)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of France?\n",
      "Answer: godfather Menu entirety 10,826 lineup\n",
      "\n",
      "Question: Who wrote the Iliad?\n",
      "Answer: Educación Gerhard OEMs\n",
      "\n",
      "Question: When was the Declaration of Independence signed?\n",
      "Answer: godfather Menu entirety 10,826 lineup génos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test chatbot by interacting with Sample Questions\n",
    "sample_questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who wrote the Iliad?\",\n",
    "    \"When was the Declaration of Independence signed?\"\n",
    "]\n",
    "\n",
    "for q in sample_questions:\n",
    "    answer = generate_answer(seq2seq, q)\n",
    "    print(f\"Question: {q}\")\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
